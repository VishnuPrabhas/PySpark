{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create RDDs in three different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:40:22.689801Z",
     "start_time": "2021-03-31T06:40:22.131288Z"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:40:37.411818Z",
     "start_time": "2021-03-31T06:40:24.867317Z"
    }
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"Assignment\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:40:55.088379Z",
     "start_time": "2021-03-31T06:40:55.072970Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.session.SparkSession"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:40:58.084540Z",
     "start_time": "2021-03-31T06:40:57.630535Z"
    }
   },
   "outputs": [],
   "source": [
    "# RDD using parallelize method\n",
    "rdd_par = spark.sparkContext.parallelize([\"This is to create a RDD\", 'Using three different methods', \n",
    "                                         \"RDD_Parallelize, RDD_Transformations, RDD_Using_Data_Sources\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:40:59.421354Z",
     "start_time": "2021-03-31T06:40:59.407329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(rdd_par)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:41:04.005417Z",
     "start_time": "2021-03-31T06:41:00.412709Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This is to create a RDD',\n",
       " 'Using three different methods',\n",
       " 'RDD_Parallelize, RDD_Transformations, RDD_Using_Data_Sources']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_par.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-03-31T06:42:39.855364Z",
     "start_time": "2021-03-31T06:41:41.161572Z"
    }
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 22) (LAPTOP-09CV6DDO executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-e08291c8fa4e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# RDD using transformation method, creating using ezisting rdd_par\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mrdd_trans\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrdd_par\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrdd_trans\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    947\u001b[0m         \"\"\"\n\u001b[0;32m    948\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 949\u001b[1;33m             \u001b[0msock_info\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    950\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 6 in stage 2.0 failed 1 times, most recent failure: Lost task 6.0 in stage 2.0 (TID 22) (LAPTOP-09CV6DDO executor driver): org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2253)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2202)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2201)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2201)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1078)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1078)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2440)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2382)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2371)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:868)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2202)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2223)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2242)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2267)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1030)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:414)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1029)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:180)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Python worker failed to connect back.\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:182)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.create(PythonWorkerFactory.scala:107)\r\n\tat org.apache.spark.SparkEnv.createPythonWorker(SparkEnv.scala:119)\r\n\tat org.apache.spark.api.python.BasePythonRunner.compute(PythonRunner.scala:145)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:65)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:373)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:337)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:131)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:497)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1439)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:500)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\nCaused by: java.net.SocketTimeoutException: Accept timed out\r\n\tat java.net.DualStackPlainSocketImpl.waitForNewConnection(Native Method)\r\n\tat java.net.DualStackPlainSocketImpl.socketAccept(Unknown Source)\r\n\tat java.net.AbstractPlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.PlainSocketImpl.accept(Unknown Source)\r\n\tat java.net.ServerSocket.implAccept(Unknown Source)\r\n\tat java.net.ServerSocket.accept(Unknown Source)\r\n\tat org.apache.spark.api.python.PythonWorkerFactory.createSimpleWorker(PythonWorkerFactory.scala:174)\r\n\t... 14 more\r\n"
     ]
    }
   ],
   "source": [
    "# RDD using transformation method, creating using ezisting rdd_par\n",
    "rdd_trans = rdd_par.filter(lambda word:word.startswith('r'))\n",
    "rdd_trans.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RDD using Data Sources\n",
    "rdd_ds = spark.sparkContext.textFile('India.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Read a text file and count the Number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"India's history and culture is dynamic, spanning back to the beginning of human civilization. It begins with a mysterious culture along the Indus River and in farming communities in the southern lands of India. The history of India is punctuated by constant integration of migrating people with the diverse cultures that surround India. Available evidence suggests that the use of iron, copper and other metals was widely prevalent in the Indian sub-continent at a fairly early period, which is indicative of the progress that this part of the world had made. By the end of the fourth millennium BC, India had emerged as a region of highly developed civilization.\",\n",
       " '',\n",
       " '',\n",
       " 'The Indus Valley Civilization',\n",
       " 'The History of India begins with the birth of the Indus Valley Civilization, more precisely known as Harappan Civilization. It flourished around 2,500 BC, in the western part of South Asia, what today is Pakistan and Western India. The Indus Valley was home to the largest of the four ancient urban civilizations of Egypt, Mesopotamia, India and China. Nothing was known about this civilization till 1920s when the Archaeological Department of India carried out excavations in the Indus valley wherein the ruins of the two old cities, viz. Mohenjodaro and Harappa were unearthed. The ruins of buildings and other things like household articles, weapons of war, gold and silver ornaments, seals, toys, pottery wares, etc., show that some four to five thousand years ago a highly developed Civilization flourished in this region.',\n",
       " 'The Indus valley civilization was basically an urban civilization and the people lived in well-planned and well-built towns, which were also the centers for trade. The ruins of Mohenjodaro and Harappa show that these were magnificent merchant cities-well planned, scientifically laid, and well looked after. They had wide roads and a well-developed drainage system. The houses were made of baked bricks and had two or more storeys.',\n",
       " 'The highly civilized Harappans knew the art of growing cereals, and wheat and barley constituted their staple food. They consumed vegetables and fruits and ate mutton, pork and eggs as well. Evidences also show that they wore cotton as well as woollen garments. By 1500 BC, the Harappan culture came to an end. Among various causes ascribed to the decay of Indus Valley Civilization are the recurrent floods and other natural causes like earthquake, etc.',\n",
       " '',\n",
       " '',\n",
       " 'Vedic Civilization',\n",
       " 'The Vedic civilization is the earliest civilization in the history of ancient India. It is named after the Vedas, the early literature of the Hindu people. The Vedic Civilization flourished along the river Saraswati, in a region that now consists of the modern Indian states of Haryana and Punjab. Vedic is synonymous with Hinduism, which is another name for religious and spiritual thought that has evolved from the Vedas.',\n",
       " 'The Ramayana and Mahabharata were the two great epics of this period.',\n",
       " '']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_ds.collect() #reading txt file India"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "475"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(rdd_ds.count())\n",
    "rdd_ds.flatMap(lambda word: word.split(' ')).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Write a Program to find the word frequency in a given file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(\"India's\", 1),\n",
       " ('is', 8),\n",
       " ('spanning', 1),\n",
       " ('of', 27),\n",
       " ('human', 1),\n",
       " ('It', 3),\n",
       " ('begins', 2),\n",
       " ('mysterious', 1),\n",
       " ('in', 9),\n",
       " ('farming', 1),\n",
       " ('communities', 1),\n",
       " ('The', 12),\n",
       " ('India', 5),\n",
       " ('integration', 1),\n",
       " ('migrating', 1),\n",
       " ('diverse', 1),\n",
       " ('cultures', 1),\n",
       " ('evidence', 1),\n",
       " ('use', 1),\n",
       " ('iron,', 1),\n",
       " ('copper', 1),\n",
       " ('other', 3),\n",
       " ('metals', 1),\n",
       " ('was', 4),\n",
       " ('widely', 1),\n",
       " ('Indian', 2),\n",
       " ('at', 1),\n",
       " ('fairly', 1),\n",
       " ('period,', 1),\n",
       " ('progress', 1),\n",
       " ('this', 4),\n",
       " ('world', 1),\n",
       " ('made.', 1),\n",
       " ('end', 1),\n",
       " ('fourth', 1),\n",
       " ('millennium', 1),\n",
       " ('BC,', 3),\n",
       " ('as', 5),\n",
       " ('region', 2),\n",
       " ('developed', 2),\n",
       " ('', 5),\n",
       " ('Valley', 4),\n",
       " ('Civilization', 5),\n",
       " ('Civilization,', 1),\n",
       " ('more', 2),\n",
       " ('precisely', 1),\n",
       " ('known', 2),\n",
       " ('Civilization.', 1),\n",
       " ('around', 1),\n",
       " ('2,500', 1),\n",
       " ('Asia,', 1),\n",
       " ('Pakistan', 1),\n",
       " ('Western', 1),\n",
       " ('home', 1),\n",
       " ('four', 2),\n",
       " ('ancient', 2),\n",
       " ('civilizations', 1),\n",
       " ('China.', 1),\n",
       " ('Nothing', 1),\n",
       " ('civilization', 5),\n",
       " ('till', 1),\n",
       " ('when', 1),\n",
       " ('carried', 1),\n",
       " ('out', 1),\n",
       " ('excavations', 1),\n",
       " ('valley', 2),\n",
       " ('two', 3),\n",
       " ('cities,', 1),\n",
       " ('Mohenjodaro', 2),\n",
       " ('unearthed.', 1),\n",
       " ('buildings', 1),\n",
       " ('things', 1),\n",
       " ('like', 2),\n",
       " ('weapons', 1),\n",
       " ('war,', 1),\n",
       " ('gold', 1),\n",
       " ('silver', 1),\n",
       " ('toys,', 1),\n",
       " ('wares,', 1),\n",
       " ('etc.,', 1),\n",
       " ('thousand', 1),\n",
       " ('years', 1),\n",
       " ('ago', 1),\n",
       " ('region.', 1),\n",
       " ('basically', 1),\n",
       " ('an', 2),\n",
       " ('lived', 1),\n",
       " ('well-planned', 1),\n",
       " ('centers', 1),\n",
       " ('trade.', 1),\n",
       " ('these', 1),\n",
       " ('merchant', 1),\n",
       " ('looked', 1),\n",
       " ('They', 2),\n",
       " ('roads', 1),\n",
       " ('houses', 1),\n",
       " ('baked', 1),\n",
       " ('civilized', 1),\n",
       " ('cereals,', 1),\n",
       " ('wheat', 1),\n",
       " ('barley', 1),\n",
       " ('food.', 1),\n",
       " ('consumed', 1),\n",
       " ('mutton,', 1),\n",
       " ('came', 1),\n",
       " ('Among', 1),\n",
       " ('causes', 2),\n",
       " ('are', 1),\n",
       " ('recurrent', 1),\n",
       " ('etc.', 1),\n",
       " ('Vedic', 4),\n",
       " ('earliest', 1),\n",
       " ('named', 1),\n",
       " ('after', 1),\n",
       " ('Vedas,', 1),\n",
       " ('Hindu', 1),\n",
       " ('people.', 1),\n",
       " ('now', 1),\n",
       " ('modern', 1),\n",
       " ('Haryana', 1),\n",
       " ('Punjab.', 1),\n",
       " ('synonymous', 1),\n",
       " ('name', 1),\n",
       " ('religious', 1),\n",
       " ('thought', 1),\n",
       " ('Vedas.', 1),\n",
       " ('Ramayana', 1),\n",
       " ('history', 3),\n",
       " ('and', 23),\n",
       " ('culture', 3),\n",
       " ('dynamic,', 1),\n",
       " ('back', 1),\n",
       " ('to', 5),\n",
       " ('the', 34),\n",
       " ('beginning', 1),\n",
       " ('civilization.', 2),\n",
       " ('with', 4),\n",
       " ('a', 6),\n",
       " ('along', 2),\n",
       " ('Indus', 7),\n",
       " ('River', 1),\n",
       " ('southern', 1),\n",
       " ('lands', 1),\n",
       " ('India.', 4),\n",
       " ('punctuated', 1),\n",
       " ('by', 1),\n",
       " ('constant', 1),\n",
       " ('people', 2),\n",
       " ('that', 8),\n",
       " ('surround', 1),\n",
       " ('Available', 1),\n",
       " ('suggests', 1),\n",
       " ('prevalent', 1),\n",
       " ('sub-continent', 1),\n",
       " ('early', 2),\n",
       " ('which', 3),\n",
       " ('indicative', 1),\n",
       " ('part', 2),\n",
       " ('had', 4),\n",
       " ('By', 2),\n",
       " ('emerged', 1),\n",
       " ('highly', 3),\n",
       " ('History', 1),\n",
       " ('birth', 1),\n",
       " ('Harappan', 2),\n",
       " ('flourished', 3),\n",
       " ('western', 1),\n",
       " ('South', 1),\n",
       " ('what', 1),\n",
       " ('today', 1),\n",
       " ('largest', 1),\n",
       " ('urban', 2),\n",
       " ('Egypt,', 1),\n",
       " ('Mesopotamia,', 1),\n",
       " ('about', 1),\n",
       " ('1920s', 1),\n",
       " ('Archaeological', 1),\n",
       " ('Department', 1),\n",
       " ('wherein', 1),\n",
       " ('ruins', 3),\n",
       " ('old', 1),\n",
       " ('viz.', 1),\n",
       " ('Harappa', 2),\n",
       " ('were', 5),\n",
       " ('household', 1),\n",
       " ('articles,', 1),\n",
       " ('ornaments,', 1),\n",
       " ('seals,', 1),\n",
       " ('pottery', 1),\n",
       " ('show', 3),\n",
       " ('some', 1),\n",
       " ('five', 1),\n",
       " ('well-built', 1),\n",
       " ('towns,', 1),\n",
       " ('also', 2),\n",
       " ('for', 2),\n",
       " ('magnificent', 1),\n",
       " ('cities-well', 1),\n",
       " ('planned,', 1),\n",
       " ('scientifically', 1),\n",
       " ('laid,', 1),\n",
       " ('well', 2),\n",
       " ('after.', 1),\n",
       " ('wide', 1),\n",
       " ('well-developed', 1),\n",
       " ('drainage', 1),\n",
       " ('system.', 1),\n",
       " ('made', 1),\n",
       " ('bricks', 1),\n",
       " ('or', 1),\n",
       " ('storeys.', 1),\n",
       " ('Harappans', 1),\n",
       " ('knew', 1),\n",
       " ('art', 1),\n",
       " ('growing', 1),\n",
       " ('constituted', 1),\n",
       " ('their', 1),\n",
       " ('staple', 1),\n",
       " ('vegetables', 1),\n",
       " ('fruits', 1),\n",
       " ('ate', 1),\n",
       " ('pork', 1),\n",
       " ('eggs', 1),\n",
       " ('well.', 1),\n",
       " ('Evidences', 1),\n",
       " ('they', 1),\n",
       " ('wore', 1),\n",
       " ('cotton', 1),\n",
       " ('woollen', 1),\n",
       " ('garments.', 1),\n",
       " ('1500', 1),\n",
       " ('end.', 1),\n",
       " ('various', 1),\n",
       " ('ascribed', 1),\n",
       " ('decay', 1),\n",
       " ('floods', 1),\n",
       " ('natural', 1),\n",
       " ('earthquake,', 1),\n",
       " ('literature', 1),\n",
       " ('river', 1),\n",
       " ('Saraswati,', 1),\n",
       " ('consists', 1),\n",
       " ('states', 1),\n",
       " ('Hinduism,', 1),\n",
       " ('another', 1),\n",
       " ('spiritual', 1),\n",
       " ('has', 1),\n",
       " ('evolved', 1),\n",
       " ('from', 1),\n",
       " ('Mahabharata', 1),\n",
       " ('great', 1),\n",
       " ('epics', 1),\n",
       " ('period.', 1)]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rdd = rdd_ds.flatMap(lambda word: word.split(' '))\n",
    "freq_words = word_rdd.map(lambda word: (word, 1))\n",
    "freq_words.reduceByKey(lambda a, b: a + b).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write a program to convert all words in a file to upper case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"INDIA'S HISTORY AND CULTURE IS DYNAMIC, SPANNING BACK TO THE BEGINNING OF HUMAN CIVILIZATION. IT BEGINS WITH A MYSTERIOUS CULTURE ALONG THE INDUS RIVER AND IN FARMING COMMUNITIES IN THE SOUTHERN LANDS OF INDIA. THE HISTORY OF INDIA IS PUNCTUATED BY CONSTANT INTEGRATION OF MIGRATING PEOPLE WITH THE DIVERSE CULTURES THAT SURROUND INDIA. AVAILABLE EVIDENCE SUGGESTS THAT THE USE OF IRON, COPPER AND OTHER METALS WAS WIDELY PREVALENT IN THE INDIAN SUB-CONTINENT AT A FAIRLY EARLY PERIOD, WHICH IS INDICATIVE OF THE PROGRESS THAT THIS PART OF THE WORLD HAD MADE. BY THE END OF THE FOURTH MILLENNIUM BC, INDIA HAD EMERGED AS A REGION OF HIGHLY DEVELOPED CIVILIZATION.\",\n",
       " '',\n",
       " '',\n",
       " 'THE INDUS VALLEY CIVILIZATION',\n",
       " 'THE HISTORY OF INDIA BEGINS WITH THE BIRTH OF THE INDUS VALLEY CIVILIZATION, MORE PRECISELY KNOWN AS HARAPPAN CIVILIZATION. IT FLOURISHED AROUND 2,500 BC, IN THE WESTERN PART OF SOUTH ASIA, WHAT TODAY IS PAKISTAN AND WESTERN INDIA. THE INDUS VALLEY WAS HOME TO THE LARGEST OF THE FOUR ANCIENT URBAN CIVILIZATIONS OF EGYPT, MESOPOTAMIA, INDIA AND CHINA. NOTHING WAS KNOWN ABOUT THIS CIVILIZATION TILL 1920S WHEN THE ARCHAEOLOGICAL DEPARTMENT OF INDIA CARRIED OUT EXCAVATIONS IN THE INDUS VALLEY WHEREIN THE RUINS OF THE TWO OLD CITIES, VIZ. MOHENJODARO AND HARAPPA WERE UNEARTHED. THE RUINS OF BUILDINGS AND OTHER THINGS LIKE HOUSEHOLD ARTICLES, WEAPONS OF WAR, GOLD AND SILVER ORNAMENTS, SEALS, TOYS, POTTERY WARES, ETC., SHOW THAT SOME FOUR TO FIVE THOUSAND YEARS AGO A HIGHLY DEVELOPED CIVILIZATION FLOURISHED IN THIS REGION.',\n",
       " 'THE INDUS VALLEY CIVILIZATION WAS BASICALLY AN URBAN CIVILIZATION AND THE PEOPLE LIVED IN WELL-PLANNED AND WELL-BUILT TOWNS, WHICH WERE ALSO THE CENTERS FOR TRADE. THE RUINS OF MOHENJODARO AND HARAPPA SHOW THAT THESE WERE MAGNIFICENT MERCHANT CITIES-WELL PLANNED, SCIENTIFICALLY LAID, AND WELL LOOKED AFTER. THEY HAD WIDE ROADS AND A WELL-DEVELOPED DRAINAGE SYSTEM. THE HOUSES WERE MADE OF BAKED BRICKS AND HAD TWO OR MORE STOREYS.',\n",
       " 'THE HIGHLY CIVILIZED HARAPPANS KNEW THE ART OF GROWING CEREALS, AND WHEAT AND BARLEY CONSTITUTED THEIR STAPLE FOOD. THEY CONSUMED VEGETABLES AND FRUITS AND ATE MUTTON, PORK AND EGGS AS WELL. EVIDENCES ALSO SHOW THAT THEY WORE COTTON AS WELL AS WOOLLEN GARMENTS. BY 1500 BC, THE HARAPPAN CULTURE CAME TO AN END. AMONG VARIOUS CAUSES ASCRIBED TO THE DECAY OF INDUS VALLEY CIVILIZATION ARE THE RECURRENT FLOODS AND OTHER NATURAL CAUSES LIKE EARTHQUAKE, ETC.',\n",
       " '',\n",
       " '',\n",
       " 'VEDIC CIVILIZATION',\n",
       " 'THE VEDIC CIVILIZATION IS THE EARLIEST CIVILIZATION IN THE HISTORY OF ANCIENT INDIA. IT IS NAMED AFTER THE VEDAS, THE EARLY LITERATURE OF THE HINDU PEOPLE. THE VEDIC CIVILIZATION FLOURISHED ALONG THE RIVER SARASWATI, IN A REGION THAT NOW CONSISTS OF THE MODERN INDIAN STATES OF HARYANA AND PUNJAB. VEDIC IS SYNONYMOUS WITH HINDUISM, WHICH IS ANOTHER NAME FOR RELIGIOUS AND SPIRITUAL THOUGHT THAT HAS EVOLVED FROM THE VEDAS.',\n",
       " 'THE RAMAYANA AND MAHABHARATA WERE THE TWO GREAT EPICS OF THIS PERIOD.',\n",
       " '']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_upper = rdd_ds.map(lambda word: word.upper())\n",
    "rdd_upper.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Write a program to convert all words in a file to lower case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"india's history and culture is dynamic, spanning back to the beginning of human civilization. it begins with a mysterious culture along the indus river and in farming communities in the southern lands of india. the history of india is punctuated by constant integration of migrating people with the diverse cultures that surround india. available evidence suggests that the use of iron, copper and other metals was widely prevalent in the indian sub-continent at a fairly early period, which is indicative of the progress that this part of the world had made. by the end of the fourth millennium bc, india had emerged as a region of highly developed civilization.\",\n",
       " '',\n",
       " '',\n",
       " 'the indus valley civilization',\n",
       " 'the history of india begins with the birth of the indus valley civilization, more precisely known as harappan civilization. it flourished around 2,500 bc, in the western part of south asia, what today is pakistan and western india. the indus valley was home to the largest of the four ancient urban civilizations of egypt, mesopotamia, india and china. nothing was known about this civilization till 1920s when the archaeological department of india carried out excavations in the indus valley wherein the ruins of the two old cities, viz. mohenjodaro and harappa were unearthed. the ruins of buildings and other things like household articles, weapons of war, gold and silver ornaments, seals, toys, pottery wares, etc., show that some four to five thousand years ago a highly developed civilization flourished in this region.',\n",
       " 'the indus valley civilization was basically an urban civilization and the people lived in well-planned and well-built towns, which were also the centers for trade. the ruins of mohenjodaro and harappa show that these were magnificent merchant cities-well planned, scientifically laid, and well looked after. they had wide roads and a well-developed drainage system. the houses were made of baked bricks and had two or more storeys.',\n",
       " 'the highly civilized harappans knew the art of growing cereals, and wheat and barley constituted their staple food. they consumed vegetables and fruits and ate mutton, pork and eggs as well. evidences also show that they wore cotton as well as woollen garments. by 1500 bc, the harappan culture came to an end. among various causes ascribed to the decay of indus valley civilization are the recurrent floods and other natural causes like earthquake, etc.',\n",
       " '',\n",
       " '',\n",
       " 'vedic civilization',\n",
       " 'the vedic civilization is the earliest civilization in the history of ancient india. it is named after the vedas, the early literature of the hindu people. the vedic civilization flourished along the river saraswati, in a region that now consists of the modern indian states of haryana and punjab. vedic is synonymous with hinduism, which is another name for religious and spiritual thought that has evolved from the vedas.',\n",
       " 'the ramayana and mahabharata were the two great epics of this period.',\n",
       " '']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_lower = rdd_ds.map(lambda word: word.lower())\n",
    "rdd_lower.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Write a program to capatilize first letter of each words in file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"India's\",\n",
       " 'History',\n",
       " 'And',\n",
       " 'Culture',\n",
       " 'Is',\n",
       " 'Dynamic,',\n",
       " 'Spanning',\n",
       " 'Back',\n",
       " 'To',\n",
       " 'The',\n",
       " 'Beginning',\n",
       " 'Of',\n",
       " 'Human',\n",
       " 'Civilization.',\n",
       " 'It',\n",
       " 'Begins',\n",
       " 'With',\n",
       " 'A',\n",
       " 'Mysterious',\n",
       " 'Culture',\n",
       " 'Along',\n",
       " 'The',\n",
       " 'Indus',\n",
       " 'River',\n",
       " 'And',\n",
       " 'In',\n",
       " 'Farming',\n",
       " 'Communities',\n",
       " 'In',\n",
       " 'The',\n",
       " 'Southern',\n",
       " 'Lands',\n",
       " 'Of',\n",
       " 'India.',\n",
       " 'The',\n",
       " 'History',\n",
       " 'Of',\n",
       " 'India',\n",
       " 'Is',\n",
       " 'Punctuated',\n",
       " 'By',\n",
       " 'Constant',\n",
       " 'Integration',\n",
       " 'Of',\n",
       " 'Migrating',\n",
       " 'People',\n",
       " 'With',\n",
       " 'The',\n",
       " 'Diverse',\n",
       " 'Cultures',\n",
       " 'That',\n",
       " 'Surround',\n",
       " 'India.',\n",
       " 'Available',\n",
       " 'Evidence',\n",
       " 'Suggests',\n",
       " 'That',\n",
       " 'The',\n",
       " 'Use',\n",
       " 'Of',\n",
       " 'Iron,',\n",
       " 'Copper',\n",
       " 'And',\n",
       " 'Other',\n",
       " 'Metals',\n",
       " 'Was',\n",
       " 'Widely',\n",
       " 'Prevalent',\n",
       " 'In',\n",
       " 'The',\n",
       " 'Indian',\n",
       " 'Sub-continent',\n",
       " 'At',\n",
       " 'A',\n",
       " 'Fairly',\n",
       " 'Early',\n",
       " 'Period,',\n",
       " 'Which',\n",
       " 'Is',\n",
       " 'Indicative',\n",
       " 'Of',\n",
       " 'The',\n",
       " 'Progress',\n",
       " 'That',\n",
       " 'This',\n",
       " 'Part',\n",
       " 'Of',\n",
       " 'The',\n",
       " 'World',\n",
       " 'Had',\n",
       " 'Made.',\n",
       " 'By',\n",
       " 'The',\n",
       " 'End',\n",
       " 'Of',\n",
       " 'The',\n",
       " 'Fourth',\n",
       " 'Millennium',\n",
       " 'Bc,',\n",
       " 'India',\n",
       " 'Had',\n",
       " 'Emerged',\n",
       " 'As',\n",
       " 'A',\n",
       " 'Region',\n",
       " 'Of',\n",
       " 'Highly',\n",
       " 'Developed',\n",
       " 'Civilization.',\n",
       " '',\n",
       " '',\n",
       " 'The',\n",
       " 'Indus',\n",
       " 'Valley',\n",
       " 'Civilization',\n",
       " 'The',\n",
       " 'History',\n",
       " 'Of',\n",
       " 'India',\n",
       " 'Begins',\n",
       " 'With',\n",
       " 'The',\n",
       " 'Birth',\n",
       " 'Of',\n",
       " 'The',\n",
       " 'Indus',\n",
       " 'Valley',\n",
       " 'Civilization,',\n",
       " 'More',\n",
       " 'Precisely',\n",
       " 'Known',\n",
       " 'As',\n",
       " 'Harappan',\n",
       " 'Civilization.',\n",
       " 'It',\n",
       " 'Flourished',\n",
       " 'Around',\n",
       " '2,500',\n",
       " 'Bc,',\n",
       " 'In',\n",
       " 'The',\n",
       " 'Western',\n",
       " 'Part',\n",
       " 'Of',\n",
       " 'South',\n",
       " 'Asia,',\n",
       " 'What',\n",
       " 'Today',\n",
       " 'Is',\n",
       " 'Pakistan',\n",
       " 'And',\n",
       " 'Western',\n",
       " 'India.',\n",
       " 'The',\n",
       " 'Indus',\n",
       " 'Valley',\n",
       " 'Was',\n",
       " 'Home',\n",
       " 'To',\n",
       " 'The',\n",
       " 'Largest',\n",
       " 'Of',\n",
       " 'The',\n",
       " 'Four',\n",
       " 'Ancient',\n",
       " 'Urban',\n",
       " 'Civilizations',\n",
       " 'Of',\n",
       " 'Egypt,',\n",
       " 'Mesopotamia,',\n",
       " 'India',\n",
       " 'And',\n",
       " 'China.',\n",
       " 'Nothing',\n",
       " 'Was',\n",
       " 'Known',\n",
       " 'About',\n",
       " 'This',\n",
       " 'Civilization',\n",
       " 'Till',\n",
       " '1920s',\n",
       " 'When',\n",
       " 'The',\n",
       " 'Archaeological',\n",
       " 'Department',\n",
       " 'Of',\n",
       " 'India',\n",
       " 'Carried',\n",
       " 'Out',\n",
       " 'Excavations',\n",
       " 'In',\n",
       " 'The',\n",
       " 'Indus',\n",
       " 'Valley',\n",
       " 'Wherein',\n",
       " 'The',\n",
       " 'Ruins',\n",
       " 'Of',\n",
       " 'The',\n",
       " 'Two',\n",
       " 'Old',\n",
       " 'Cities,',\n",
       " 'Viz.',\n",
       " 'Mohenjodaro',\n",
       " 'And',\n",
       " 'Harappa',\n",
       " 'Were',\n",
       " 'Unearthed.',\n",
       " 'The',\n",
       " 'Ruins',\n",
       " 'Of',\n",
       " 'Buildings',\n",
       " 'And',\n",
       " 'Other',\n",
       " 'Things',\n",
       " 'Like',\n",
       " 'Household',\n",
       " 'Articles,',\n",
       " 'Weapons',\n",
       " 'Of',\n",
       " 'War,',\n",
       " 'Gold',\n",
       " 'And',\n",
       " 'Silver',\n",
       " 'Ornaments,',\n",
       " 'Seals,',\n",
       " 'Toys,',\n",
       " 'Pottery',\n",
       " 'Wares,',\n",
       " 'Etc.,',\n",
       " 'Show',\n",
       " 'That',\n",
       " 'Some',\n",
       " 'Four',\n",
       " 'To',\n",
       " 'Five',\n",
       " 'Thousand',\n",
       " 'Years',\n",
       " 'Ago',\n",
       " 'A',\n",
       " 'Highly',\n",
       " 'Developed',\n",
       " 'Civilization',\n",
       " 'Flourished',\n",
       " 'In',\n",
       " 'This',\n",
       " 'Region.',\n",
       " 'The',\n",
       " 'Indus',\n",
       " 'Valley',\n",
       " 'Civilization',\n",
       " 'Was',\n",
       " 'Basically',\n",
       " 'An',\n",
       " 'Urban',\n",
       " 'Civilization',\n",
       " 'And',\n",
       " 'The',\n",
       " 'People',\n",
       " 'Lived',\n",
       " 'In',\n",
       " 'Well-planned',\n",
       " 'And',\n",
       " 'Well-built',\n",
       " 'Towns,',\n",
       " 'Which',\n",
       " 'Were',\n",
       " 'Also',\n",
       " 'The',\n",
       " 'Centers',\n",
       " 'For',\n",
       " 'Trade.',\n",
       " 'The',\n",
       " 'Ruins',\n",
       " 'Of',\n",
       " 'Mohenjodaro',\n",
       " 'And',\n",
       " 'Harappa',\n",
       " 'Show',\n",
       " 'That',\n",
       " 'These',\n",
       " 'Were',\n",
       " 'Magnificent',\n",
       " 'Merchant',\n",
       " 'Cities-well',\n",
       " 'Planned,',\n",
       " 'Scientifically',\n",
       " 'Laid,',\n",
       " 'And',\n",
       " 'Well',\n",
       " 'Looked',\n",
       " 'After.',\n",
       " 'They',\n",
       " 'Had',\n",
       " 'Wide',\n",
       " 'Roads',\n",
       " 'And',\n",
       " 'A',\n",
       " 'Well-developed',\n",
       " 'Drainage',\n",
       " 'System.',\n",
       " 'The',\n",
       " 'Houses',\n",
       " 'Were',\n",
       " 'Made',\n",
       " 'Of',\n",
       " 'Baked',\n",
       " 'Bricks',\n",
       " 'And',\n",
       " 'Had',\n",
       " 'Two',\n",
       " 'Or',\n",
       " 'More',\n",
       " 'Storeys.',\n",
       " 'The',\n",
       " 'Highly',\n",
       " 'Civilized',\n",
       " 'Harappans',\n",
       " 'Knew',\n",
       " 'The',\n",
       " 'Art',\n",
       " 'Of',\n",
       " 'Growing',\n",
       " 'Cereals,',\n",
       " 'And',\n",
       " 'Wheat',\n",
       " 'And',\n",
       " 'Barley',\n",
       " 'Constituted',\n",
       " 'Their',\n",
       " 'Staple',\n",
       " 'Food.',\n",
       " 'They',\n",
       " 'Consumed',\n",
       " 'Vegetables',\n",
       " 'And',\n",
       " 'Fruits',\n",
       " 'And',\n",
       " 'Ate',\n",
       " 'Mutton,',\n",
       " 'Pork',\n",
       " 'And',\n",
       " 'Eggs',\n",
       " 'As',\n",
       " 'Well.',\n",
       " 'Evidences',\n",
       " 'Also',\n",
       " 'Show',\n",
       " 'That',\n",
       " 'They',\n",
       " 'Wore',\n",
       " 'Cotton',\n",
       " 'As',\n",
       " 'Well',\n",
       " 'As',\n",
       " 'Woollen',\n",
       " 'Garments.',\n",
       " 'By',\n",
       " '1500',\n",
       " 'Bc,',\n",
       " 'The',\n",
       " 'Harappan',\n",
       " 'Culture',\n",
       " 'Came',\n",
       " 'To',\n",
       " 'An',\n",
       " 'End.',\n",
       " 'Among',\n",
       " 'Various',\n",
       " 'Causes',\n",
       " 'Ascribed',\n",
       " 'To',\n",
       " 'The',\n",
       " 'Decay',\n",
       " 'Of',\n",
       " 'Indus',\n",
       " 'Valley',\n",
       " 'Civilization',\n",
       " 'Are',\n",
       " 'The',\n",
       " 'Recurrent',\n",
       " 'Floods',\n",
       " 'And',\n",
       " 'Other',\n",
       " 'Natural',\n",
       " 'Causes',\n",
       " 'Like',\n",
       " 'Earthquake,',\n",
       " 'Etc.',\n",
       " '',\n",
       " '',\n",
       " 'Vedic',\n",
       " 'Civilization',\n",
       " 'The',\n",
       " 'Vedic',\n",
       " 'Civilization',\n",
       " 'Is',\n",
       " 'The',\n",
       " 'Earliest',\n",
       " 'Civilization',\n",
       " 'In',\n",
       " 'The',\n",
       " 'History',\n",
       " 'Of',\n",
       " 'Ancient',\n",
       " 'India.',\n",
       " 'It',\n",
       " 'Is',\n",
       " 'Named',\n",
       " 'After',\n",
       " 'The',\n",
       " 'Vedas,',\n",
       " 'The',\n",
       " 'Early',\n",
       " 'Literature',\n",
       " 'Of',\n",
       " 'The',\n",
       " 'Hindu',\n",
       " 'People.',\n",
       " 'The',\n",
       " 'Vedic',\n",
       " 'Civilization',\n",
       " 'Flourished',\n",
       " 'Along',\n",
       " 'The',\n",
       " 'River',\n",
       " 'Saraswati,',\n",
       " 'In',\n",
       " 'A',\n",
       " 'Region',\n",
       " 'That',\n",
       " 'Now',\n",
       " 'Consists',\n",
       " 'Of',\n",
       " 'The',\n",
       " 'Modern',\n",
       " 'Indian',\n",
       " 'States',\n",
       " 'Of',\n",
       " 'Haryana',\n",
       " 'And',\n",
       " 'Punjab.',\n",
       " 'Vedic',\n",
       " 'Is',\n",
       " 'Synonymous',\n",
       " 'With',\n",
       " 'Hinduism,',\n",
       " 'Which',\n",
       " 'Is',\n",
       " 'Another',\n",
       " 'Name',\n",
       " 'For',\n",
       " 'Religious',\n",
       " 'And',\n",
       " 'Spiritual',\n",
       " 'Thought',\n",
       " 'That',\n",
       " 'Has',\n",
       " 'Evolved',\n",
       " 'From',\n",
       " 'The',\n",
       " 'Vedas.',\n",
       " 'The',\n",
       " 'Ramayana',\n",
       " 'And',\n",
       " 'Mahabharata',\n",
       " 'Were',\n",
       " 'The',\n",
       " 'Two',\n",
       " 'Great',\n",
       " 'Epics',\n",
       " 'Of',\n",
       " 'This',\n",
       " 'Period.',\n",
       " '']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_rdd = rdd_ds.flatMap(lambda word: word.split(' ')) #flatMap() is a transformation operation that flattens the RDD/DataFrame (array/map DataFrame columns) after applying the function on 'every element' and returns a new PySpark RDD/DataFrame. \n",
    "rdd_capitalized = word_rdd.map(lambda word: word.capitalize())\n",
    "rdd_capitalized.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Find the longest length of word from given set of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "long_rdd = rdd_ds.flatMap(lambda word: word.split(' '))\n",
    "Lword_rdd = long_rdd.map(lambda word: len(word))\n",
    "max(Lword_rdd.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Map the Registration numbers to corresponding branch. 6000 series BDA, 9000 series HDA, 1000 series ML, 2000 series VLSI, 3000 series ES, 4000 series MSc, 5000 series CC. Given registration number, generate a key-value pair of Registration Number and Corresponding Branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Given Registraion Number belongs to corresponding Branch: \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(6130, 'BDA'),\n",
       " (3521, 'ES'),\n",
       " (2504, 'VLSI'),\n",
       " (9450, 'HDA'),\n",
       " (1998, 'ML'),\n",
       " (5999, 'Check the Registration Number')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series = [6130, 3521, 2504, 9450, 1998, 5999]\n",
    "rdd_par = spark.sparkContext.parallelize(series)\n",
    "rdd_sequence = rdd_par.map(lambda series: (series, 'BDA') if (series >= 6000 and series < 6999)  \n",
    "                           else ((series, 'HDA') if (series >= 9000 and series < 9999)\n",
    "                           else ((series, 'ML') if (series >= 1000 and series < 1999) \n",
    "                           else ((series, 'VLSI') if (series >= 2000 and series < 2999) \n",
    "                           else ((series, 'ES') if (series >= 3000 and series < 3999) \n",
    "                           else ((series, 'MSc') if (series >= 4000 and series < 4999)\n",
    "                           else ((series, 'CC') if (series >= 5000 and series < 5999) else (series, 'Check the Registration Number'))))))))\n",
    "\n",
    "print(\"Given Registraion Number belongs to corresponding Branch: \")\n",
    "rdd_sequence.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Text file contain numbers. Numbers are separated by one white space. There is no order to store the numbers. One line may  contain one or more numbers. Find the maximum, minimum, sum and mean of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The maximum number is:  7000\n",
      "The minimum number is:  5\n",
      "The sum of numbers is:  30058\n",
      "The mean is:  1582.0\n"
     ]
    }
   ],
   "source": [
    "Rdd_ds = spark.sparkContext.textFile('test_numbers.txt')\n",
    "rdd_ds_num = Rdd_ds.flatMap(lambda n: n.split(' '))\n",
    "rdd_ds_num = rdd_ds_num.map(lambda n: int(n))\n",
    "#Rdd_ds.collect()\n",
    "\n",
    "print(\"The maximum number is: \", rdd_ds_num.max())\n",
    "print(\"The minimum number is: \", rdd_ds_num.min())\n",
    "print(\"The sum of numbers is: \", rdd_ds_num.sum())\n",
    "print(\"The mean is: \", rdd_ds_num.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. A text file (citizen.txt) contains data about citizens of country. Fields (information in file) are Name, dob, Phone, email and state name. Another file contains mapping of state names to state code like Karnataka is codes as KA, TamilNadu as TN, Kerala KL etc. Compress the citizen.txt file by changing full state name to state code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+----------+----------------+----------+\n",
      "|Name|       DoB|     Phone|           email|State Code|\n",
      "+----+----------+----------+----------------+----------+\n",
      "| ABC|11-03-1976|1234567890| test1@gmail.com|        KA|\n",
      "| DEF|02-04-1986|0987654321| test2@ymail.com|        TN|\n",
      "| GHI|20-03-1996|9874561230|test3@reddif.com|        UK|\n",
      "+----+----------+----------+----------------+----------+\n",
      "\n"
     ]
    },
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o970.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 156.0 failed 1 times, most recent failure: Lost task 1.0 in stage 156.0 (TID 1519, ADMINRG-S8CPHRR, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 223, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 212, in _batched\n    for item in iterator:\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-50-5e65a6701c61>\", line 13, in <lambda>\nTypeError: bad operand type for unary +: 'str'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2152)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 50 more\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 223, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 212, in _batched\n    for item in iterator:\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-50-5e65a6701c61>\", line 13, in <lambda>\nTypeError: bad operand type for unary +: 'str'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-5e65a6701c61>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mcitizen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatecodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'state name'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhow\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'left'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'state name'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mcitizen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcsv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'citizen_info.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mcitizen\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m  \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrepartition\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Text/citizen_information'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\rdd.py\u001b[0m in \u001b[0;36msaveAsTextFile\u001b[1;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[0;32m   1654\u001b[0m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompressionCodec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1655\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1656\u001b[1;33m             \u001b[0mkeyed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mBytesToString\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msaveAsTextFile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1658\u001b[0m     \u001b[1;31m# Pair functions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    126\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    325\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 326\u001b[1;33m                 raise Py4JJavaError(\n\u001b[0m\u001b[0;32m    327\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o970.saveAsTextFile.\n: org.apache.spark.SparkException: Job aborted.\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:100)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$3(PairRDDFunctions.scala:1008)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1007)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$2(PairRDDFunctions.scala:964)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:962)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$2(RDD.scala:1552)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1552)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$saveAsTextFile$1(RDD.scala:1538)\r\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\r\n\tat org.apache.spark.rdd.RDD.saveAsTextFile(RDD.scala:1538)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile(JavaRDDLike.scala:550)\r\n\tat org.apache.spark.api.java.JavaRDDLike.saveAsTextFile$(JavaRDDLike.scala:549)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.saveAsTextFile(JavaRDDLike.scala:45)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 156.0 failed 1 times, most recent failure: Lost task 1.0 in stage 156.0 (TID 1519, ADMINRG-S8CPHRR, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 223, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 212, in _batched\n    for item in iterator:\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-50-5e65a6701c61>\", line 13, in <lambda>\nTypeError: bad operand type for unary +: 'str'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2059)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2008)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2007)\r\n\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\r\n\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2007)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:973)\r\n\tat scala.Option.foreach(Option.scala:407)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:973)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2239)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2188)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2177)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:775)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2120)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2152)\r\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:78)\r\n\t... 50 more\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 605, in main\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 597, in process\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 223, in dump_stream\n    self.serializer.dump_stream(self._batched(iterator), stream)\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 141, in dump_stream\n    for obj in iterator:\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 212, in _batched\n    for item in iterator:\n  File \"D:\\BigData\\Spark\\spark-3.0.1-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\util.py\", line 107, in wrapper\n    return f(*args, **kwargs)\n  File \"<ipython-input-50-5e65a6701c61>\", line 13, in <lambda>\nTypeError: bad operand type for unary +: 'str'\n\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:503)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:638)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:621)\r\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:456)\r\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\r\n\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:458)\r\n\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:132)\r\n\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:52)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:127)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$3(Executor.scala:446)\r\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1377)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:449)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "rdd_ds_citizen = spark.sparkContext.textFile('citizen.txt')\n",
    "rdd_ds_statecode = spark.sparkContext.textFile('State_Code.txt')\n",
    "rdd_ds_citizen = rdd_ds_citizen.map(lambda citizen: citizen.split(', '))\n",
    "rdd_ds_statecode = rdd_ds_statecode.map(lambda state: state.split(', '))\n",
    "\n",
    "citizen = spark.createDataFrame(rdd_ds_citizen, ['Name', 'DoB', 'Phone', 'email', 'State Name'])\n",
    "statecodes = spark.createDataFrame(rdd_ds_statecode, ['State Name', 'State Code'])\n",
    "citizen.collect()\n",
    "statecodes.collect()\n",
    "\n",
    "citizen.join(statecodes, on = 'state name', how = 'left').drop('state name').show()\n",
    "citizen.write.csv('citizen_info.csv')\n",
    "citizen.rdd.map(lambda x: x[0] +  + str(x)).repartition(1).saveAsTextFile('Text/citizen_information')\n",
    "\n",
    "# PyJJavaError is about python version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
